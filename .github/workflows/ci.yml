# PipeX Continuous Integration
# Runs tests, linting, and quality checks on every push and PR

name: CI

on:
    push:
        branches: [main, develop]
    pull_request:
        branches: [main, develop]
    schedule:
        # Run tests weekly to catch dependency issues
        - cron: "0 0 * * 0"

env:
    PYTHON_VERSION: "3.11"

jobs:
    # Quick validation job
    validate:
        name: Quick Validation
        runs-on: ubuntu-latest
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v6
              with:
                  python-version: ${{ env.PYTHON_VERSION }}

            - name: Install basic dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install flake8 black isort

            - name: Check code formatting with black
              run: black --check app/ tests/

            - name: Check import sorting with isort
              run: isort --check-only app/ tests/

            - name: Quick syntax check
              run: python -m py_compile app/*.py

    # Comprehensive testing
    test:
        name: Test Suite
        runs-on: ${{ matrix.os }}
        needs: validate
        strategy:
            fail-fast: false
            matrix:
                os: [ubuntu-latest, windows-latest]
                python-version: ["3.11", "3.12"]
                include:
                    - os: macos-latest
                      python-version: "3.11"

        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python ${{ matrix.python-version }}
              uses: actions/setup-python@v6
              with:
                  python-version: ${{ matrix.python-version }}

            - name: Cache dependencies
              uses: actions/cache@v4
              with:
                  path: |
                      ~/.cache/pip
                      ~/.cache/pipx
                  key: ${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -e .[all,dev]

            - name: Run unit tests
              run: |
                  pytest tests/ -v --tb=short

            - name: Test CLI functionality
              run: |
                  pipex --help
                  pipex info

            - name: Test configuration validation
              run: |
                  # Test with example config files
                  if [ -f "config.yaml" ]; then
                    pipex validate config.yaml --dry-run || echo "Config validation test completed"
                  fi
              shell: bash

    # Code quality checks
    quality:
        name: Code Quality
        runs-on: ubuntu-latest
        needs: validate
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v6
              with:
                  python-version: ${{ env.PYTHON_VERSION }}

            - name: Install quality tools
              run: |
                  python -m pip install --upgrade pip
                  pip install -e .[dev]
                  pip install flake8 mypy bandit safety

            - name: Run flake8
              run: |
                  flake8 app/ tests/ --max-line-length=127 --extend-ignore=E203,W503

            - name: Run mypy
              run: |
                  mypy app/ --ignore-missing-imports --no-strict-optional

            - name: Security check with bandit
              run: |
                  bandit -r app/ -ll

            - name: Dependency security check
              run: |
                  safety check --ignore 70612

    # Documentation checks
    docs:
        name: Documentation
        runs-on: ubuntu-latest
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Check README links
              uses: gaurav-nelson/github-action-markdown-link-check@v1
              with:
                  use-quiet-mode: "yes"
                  use-verbose-mode: "yes"
                  config-file: ".github/mlc_config.json"

            - name: Validate YAML files
              run: |
                  python -c "
                  import yaml
                  import glob
                  for file in glob.glob('**/*.yaml', recursive=True) + glob.glob('**/*.yml', recursive=True):
                      try:
                          with open(file, 'r') as f:
                              yaml.safe_load(f)
                          print(f'✅ {file}')
                      except Exception as e:
                          print(f'❌ {file}: {e}')
                          exit(1)
                  "

    # Performance benchmarks
    benchmark:
        name: Performance Benchmark
        runs-on: ubuntu-latest
        if: github.event_name == 'pull_request'
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v6
              with:
                  python-version: ${{ env.PYTHON_VERSION }}

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -e .[all]
                  pip install pytest-benchmark

            - name: Run performance tests
              run: |
                  # Create sample data for benchmarking
                  python -c "
                  import pandas as pd
                  import numpy as np

                  # Generate test data
                  data = pd.DataFrame({
                      'id': range(10000),
                      'name': ['User_' + str(i) for i in range(10000)],
                      'value': np.random.randn(10000),
                      'category': np.random.choice(['A', 'B', 'C'], 10000)
                  })
                  data.to_csv('benchmark_data.csv', index=False)
                  print('Benchmark data created: 10K rows')
                  "

                  # Run benchmark if test file exists
                  if [ -f "tests/test_performance.py" ]; then
                    pytest tests/test_performance.py --benchmark-only
                  else
                    echo "No performance tests found, skipping benchmark"
                  fi

    # Integration tests with real services (if configured)
    integration:
        name: Integration Tests
        runs-on: ubuntu-latest
        if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[integration]')
        steps:
            - name: Checkout code
              uses: actions/checkout@v4

            - name: Set up Python
              uses: actions/setup-python@v6
              with:
                  python-version: ${{ env.PYTHON_VERSION }}

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  pip install -e .[all]

            - name: Run integration tests
              env:
                  # These would be set as repository secrets for real integration tests
                  TEST_API_TOKEN: ${{ secrets.TEST_API_TOKEN }}
                  TEST_AWS_ACCESS_KEY_ID: ${{ secrets.TEST_AWS_ACCESS_KEY_ID }}
                  TEST_AWS_SECRET_ACCESS_KEY: ${{ secrets.TEST_AWS_SECRET_ACCESS_KEY }}
              run: |
                  if [ -f "tests/test_integration.py" ]; then
                    pytest tests/test_integration.py -v
                  else
                    echo "No integration tests found"
                  fi

    # Summary job
    ci-success:
        name: CI Success
        runs-on: ubuntu-latest
        needs: [validate, test, quality, docs]
        if: always()
        steps:
            - name: Check CI Status
              run: |
                  if [ "${{ needs.validate.result }}" != "success" ] || \
                     [ "${{ needs.test.result }}" != "success" ] || \
                     [ "${{ needs.quality.result }}" != "success" ] || \
                     [ "${{ needs.docs.result }}" != "success" ]; then
                    echo "❌ CI checks failed"
                    exit 1
                  else
                    echo "✅ All CI checks passed"
                  fi
